{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Demo: Assesing hijacking of GCG suffixes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"svg\"  # 'html'\n",
    "pio.templates.default = \"plotly_white\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO remove:\n",
    "import os\n",
    "import sys\n",
    "os.getcwd()\n",
    "project_dir = '/home/sharifm/students/matanbentov/interp-jailbreak-dev/'\n",
    "os.chdir(project_dir)\n",
    "\n",
    "# os.environ[\"HF_HOME\"] = \"/home/sharifm/students/matanbentov\"  # modify to home defaco dir\n",
    "\n",
    "# Add the project directory to the sys.path to ensure Python imports from there\n",
    "sys.path.append(project_dir)\n",
    "\n",
    "from src.interp.utils import load_model\n",
    "from src.evaluate.utils import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/gemma-2-2b-it\"\n",
    "# model_name = \"qwen/qwen2.5-1.5b-instruct\"\n",
    "# model_name = \"meta-llama/llama-3-8b-instruct\"\n",
    "\n",
    "\n",
    "data_df = load_data(model_name=model_name)\n",
    "\n",
    "model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### EDAing the GCG dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\">> dataset size: {data_df.shape[0]} rows.\")\n",
    "print(f\">> {data_df.suffix_id.nunique()} different GCG suffixes, evaluated on {data_df.message_id.nunique()} messages.\")\n",
    "\n",
    "univ_scores = data_df.groupby('suffix_id').agg({'univ_score': 'first'}).sort_values(by='univ_score', ascending=False).reset_index()\n",
    "# TODO fix\n",
    "# px.bar(\n",
    "#     univ_scores,\n",
    "#     x='univ_score',\n",
    "#     title=f\"GCG suffixes' Universal Scores ({model_name})\",\n",
    "#     labels={'universal_score': 'Universal Score'},\n",
    "# ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Transformed Vectors Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO demonstrate fork usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Dominance score\n",
    "\n",
    "Quantifying the dominance score across the layers (reproducing Fig. 5 in the paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_dominance_area_plot(dom_scores, name=None):\n",
    "    ## build df:\n",
    "    df = []\n",
    "    for src_name, values in dom_scores.items():\n",
    "        for layer, val in enumerate(values):\n",
    "            df.append({\n",
    "                'src_name': src_name,\n",
    "                'layer': layer,\n",
    "                'val': val.item()\n",
    "            })\n",
    "    df = pd.DataFrame(df)\n",
    "\n",
    "    colors = {\n",
    "        'bos': '#A9A9A9',       # Dark grey\n",
    "        'chat_pre': '#D3D3D3',  # Light grey\n",
    "        'instr': '#4B77BE',     # Dark green\n",
    "        'adv': \"#E8362D\",           # Dark red\n",
    "        'chat[:-1]': '#FFA500',      # Orange\n",
    "        'chat[-1]': '#FFB84D'       # Light orange-yellow (subtler)\n",
    "    }\n",
    "\n",
    "    px.area(\n",
    "        df,\n",
    "        x='layer',\n",
    "        y='val',\n",
    "        color='src_name',\n",
    "        labels={'val': 'Dominance Score', 'layer': 'Layer', 'src_name': 'Source Subseq.'},\n",
    "        color_discrete_map=colors,\n",
    "        width=500, height=350,\n",
    "        template='plotly_white',\n",
    "        title=f\"Dominance Scores to 'chat[-1]'\" + (f' ({name})' if name else ''),\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.interp.dominance_tools import get_adv_hijack_score\n",
    "import src.interp.dominance_tools as dominance_tools\n",
    "from importlib import reload\n",
    "reload(dominance_tools)\n",
    "\n",
    "message = data_df[data_df.message_id == 619].iloc[0].message_str\n",
    "suffix_rand = data_df[data_df.suffix_category == 'init'].suffix_str.iloc[0]\n",
    "suffix_gcg = data_df[data_df.suffix_rank == 0].suffix_str.iloc[0]\n",
    "\n",
    "for suffix_name, suffix_str in [('random suffix', suffix_rand), ('GCG suffix', suffix_gcg)]:\n",
    "    dom_scores = dominance_tools.get_dom_scores(\n",
    "        model,\n",
    "        message, suffix_str,\n",
    "        dst_slc_name = 'chat[-1]',\n",
    "        hijacking_metric ='Y@attn',  # 'Y@resid', 'Y@attn', 'X@WVO@attn', 'Y@dcmp_resid', 'Y@dir\n",
    "        hijacking_metric_flavor = 'sum',  # 'sum', 'sum-top0.1'\n",
    "    )\n",
    "\n",
    "    show_dominance_area_plot(dom_scores, name=suffix_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.interp.utils import get_idx_slices\n",
    "\n",
    "\n",
    "get_idx_slices(\n",
    "        model, message, suffix_str, response_str=\"\", \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Hijacking vs Universality\n",
    "\n",
    "# TODO move to script\n",
    "\n",
    "Comparing suffixes' universality to their hijacking (repeating analysis in Fig. 8, in small scale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# TODO scale!\n",
    "\n",
    "random.seed(42)\n",
    "message_ids = random.sample(data_df.message_id.unique().tolist(), 10)\n",
    "suffix_ids = []\n",
    "intervals = [0, 0.01, 0.025] + np.arange(0.05, data_df.univ_score.max(), 0.10).tolist()\n",
    "for i in range(len(intervals) - 1):\n",
    "    interval = (intervals[i], intervals[i + 1])\n",
    "    suffixes_in_interval =  data_df[data_df.univ_score.between(*interval)].suffix_id.unique().tolist()\n",
    "    suffix_ids.extend(random.sample(\n",
    "       suffixes_in_interval, min(10, len(suffixes_in_interval))\n",
    "    ))\n",
    "\n",
    "print(f\"{len(message_ids)=}, {len(suffix_ids)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.contrib.itertools import product\n",
    "import src.interp.dominance_tools as dominance_tools\n",
    "\n",
    "df = []\n",
    "\n",
    "for message_id, suffix_id in product(message_ids, suffix_ids):\n",
    "    message_str = data_df[data_df.message_id == message_id].iloc[0].message_str\n",
    "    suffix_row = data_df[data_df.suffix_id == suffix_id].iloc[0]\n",
    "    suffix_str, suffix_univ, suffix_id = suffix_row.suffix_str, suffix_row.univ_score, suffix_row.suffix_id\n",
    "\n",
    "    dom_scores = dominance_tools.get_dom_scores(\n",
    "        model,\n",
    "        message_str, suffix_str,\n",
    "        dst_slc_name = 'chat[-1]',\n",
    "        hijacking_metric ='Y@attn',\n",
    "        hijacking_metric_flavor = 'sum',\n",
    "    )\n",
    "    hijacking_strength = dom_scores['adv'][20].item()  # adv's dominance score at layer 20 (for Gemma-2-2b-it)\n",
    "\n",
    "    df.append({\n",
    "        'message_id': message_id,\n",
    "        'suffix_id': suffix_id,\n",
    "        'message_str': message_str,\n",
    "        'suffix_str': suffix_str,\n",
    "        'suffix_univ': suffix_univ,\n",
    "        'hijacking_strength': hijacking_strength  # TODO make this dom_adv_score\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('tmp_gemma-2-2b-it-hijacking-strength.csv', index=False)\n",
    "df = pd.read_csv('results/grid_hijacking_google_gemma-2-2b-it.csv')\n",
    "df = df[(df.src == 'adv') & (df.layer == 20)].reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "agg_df = df[['suffix_id', 'suffix_univ', 'dom_score']].groupby('suffix_id').mean().reset_index()  # TODO after avg call it (=the adv dom) hijacking_strength\n",
    "spearman_corr, _ = spearmanr(agg_df['suffix_univ'], agg_df['dom_score'])\n",
    "\n",
    "px.scatter(\n",
    "    agg_df,\n",
    "    # df,\n",
    "    # color='suffix_id',\n",
    "    x='suffix_univ',\n",
    "    y='dom_score',\n",
    "    labels={'suffix_univ': \"Suffix's Universality Score\", 'dom_score': 'Hijacking Strength'},\n",
    "    title=f'Hijacking vs. Universality (Spearman={spearman_corr:.2f})',\n",
    "    width=500, height=350,\n",
    "    trendline=\"ols\",\n",
    "    # log_x=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Bin the universality scores\n",
    "bins = intervals\n",
    "bins = [0, 0.01/2, 0.01, 0.05] + np.arange(0.11, data_df.univ_score.max(), 0.10).tolist() + [data_df.univ_score.max()]\n",
    "labels = [f\"[{bins[i]:.2f}, {bins[i+1]:.2f}]\" for i in range(len(bins)-1)]\n",
    "agg_df[\"univ_bin\"] = pd.cut(agg_df[\"suffix_univ\"], bins=bins, labels=labels, include_lowest=True)\n",
    "agg_df.sort_values(by=\"univ_bin\", inplace=True)\n",
    "\n",
    "# Compute means per bin\n",
    "trend_df = agg_df.groupby(\"univ_bin\", observed=True)[\"dom_score\"].median().reset_index()\n",
    "\n",
    "# Plot boxplot using PX\n",
    "fig = px.box(\n",
    "    agg_df, x=\"univ_bin\", y=\"dom_score\", points=\"outliers\",\n",
    "    labels={\n",
    "        \"univ_bin\": \"Universality Score\",\n",
    "        \"dom_score\": \"Hijacking Strength\"\n",
    "    },\n",
    "    width=500, height=350,\n",
    "    color_discrete_sequence=[\"cornflowerblue\"]\n",
    ")\n",
    "\n",
    "# Add trendline over medians\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=trend_df[\"univ_bin\"],\n",
    "    y=trend_df[\"dom_score\"],\n",
    "    mode=\"lines+markers\",\n",
    "    line=dict(color=\"crimson\", width=3),\n",
    "    marker=dict(size=6),\n",
    "    name=\"Median Trend\"\n",
    "))\n",
    "\n",
    "# Layout tweaks\n",
    "fig.update_layout(\n",
    "    xaxis_tickangle=45,\n",
    "    showlegend=False,\n",
    ")\n",
    "\n",
    "# fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
